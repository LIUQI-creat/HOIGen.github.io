<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>HOIGen-1M: A Large-scale Dataset for Human-Object Interaction Video Generation</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">HOIGen-1M: A Large-scale Dataset for Human-Object Interaction Video Generation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <div>Kun Liu</div> </span>
              <span class="author-block">
                <div>Qi Liu</div> </span>
              <span class="author-block">
                <div>Xinchen Liu</div> </span>
              <span class="author-block">
                <div>Jie Li</div> </span>
              <span class="author-block">
                <div>Yongdong Zhang</div> </span>
              <span class="author-block">
                <div>Xiaodong He</div> </span>
              <span class="author-block">
                <div>Wu Liu</div> </span>

                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Institution Name<br>CVPR 2025</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2503.23715" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- hugging face link -->
                    <span class="link-block">
                      <a href="https://liuqi-creat.github.io/HOIGen.github.io/" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face Logo">
                      </span>
                      <span>Huggingface</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <!-- <span class="link-block">
                    <a href="https://liuqi-creat.github.io/HOIGen.github.io/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2503.23715" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="image is-16by9 mb-5">
        <img src="static/images/pic1.png" alt="Overview of HOIGen-1M">
      </div>
      <h2 class="subtitle has-text-centered">
        HOIGen-1M contains over one million video clips for HOI video generation with multiple types of HOI videos, diverse scenarios (15, 000+ objects and 7, 000+ interaction types), and expressive captions. 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Text-to-video (T2V) generation has made tremendous progress in generating complicated scenes based on texts. However, human-object interaction (HOI) often cannot be precisely generated by current T2V models due to the lack of large-scale videos with accurate captions for HOI. To address this issue, we introduce <strong>HOIGen-1M</strong>, the first large-scale dataset for <strong>HOI</strong> <strong>Gen</strong>eration, consisting of over one million high-quality videos collected from diverse sources. In particular, to guarantee the high quality of videos, we first design an efficient framework to automatically curate HOI videos using the powerful multimodal large language models (MLLMs), and then the videos are further cleaned by human annotators. Moreover, to obtain accurate textual captions for HOI videos, we design a novel video description method based on a <strong>M</strong>ixture-<strong>o</strong>f-<strong>M</strong>ultimodal-<strong>E</strong>xperts (MoME) strategy that not only generates expressive captions but also eliminates the hallucination by individual MLLM. Furthermore, due to the lack of an evaluation framework for generated HOI videos, we propose two new metrics to assess the quality of generated videos in a coarse-to-fine manner. Extensive experiments reveal that current T2V models struggle to generate high-quality HOI videos and confirm that our HOIGen-1M dataset is instrumental for improving HOI video generation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Dataset -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <h2 class="title is-3">HOIGen-1M</h2>
      <div class="columns is-centered">
          <div class="column is-full">
              <div class="content">
                  <div class="level-set has-text-justified">
                      <p>
                        The construction of a large-scale HOI dataset faces two main challenges. 
                        The first challenge is acquiring high-quality and extensive video data that includes HOI. 
                        This involves accurately sourcing videos that capture these interactions. 
                        The second challenge is obtaining high-quality captions that precisely describe the people, objects, and scenes involved. 
                        This requires accurate and detailed captions to convey the complexity of the interactions and settings depicted in the videos.
                        To address the aforementioned challenges, we build the first large-scale and high-quality dataset for HOI video generation named <strong>HOIGen-1M</strong>.
                        It exhibits three main features: 
                      <ol>
                          <li><strong>Large scale:</strong> HOIGen-1M curates over 1M video clips and all videos contain manually verified HOI, which is sufficient for training T2V models. </li>
                          <li><strong>High quality:</strong> HOIGen-1M is strictly selected from the aspects of mete attribute, aesthetics, temporal consistency, motion difference, and MLLM assessment. </li>
                          <li><strong>Expressive captions:</strong> the captions in HOIGen-1M are precise because a Mixture-of-Multimodal-Experts (MoME) strategy is employed to detect and eliminate hallucinations via cross-verification among multiple MLLMs. </li>
                      </ol>
                      </p>
                  </div>
                  <div class="container is-max-desktop">
                    <div class="hero-body">
                        <img src="static/images/table1.png" alt="A Comparison of the existing datasets for the T2V task and Our HOIGen-1M."/>
                    </div>
                    <div class="level-set has-text-justified">
                      <p>A Comparison of the existing datasets for the T2V task and Our HOIGen-1M. Our HOIGen-1M stands out as an precise T2V dataset tailored for HOI with excellent video quality and detailed captions.</p>
                    </div>
                  </div>
                  <div class="container is-max-desktop">
                    <div class="hero-body">
                        <img src="static/images/fig3.png" alt="Statistics of video clips in HOIGen-1M."/>
                    </div>
                    <div class="level-set has-text-justified">
                      <p>HOIGen-1M includes multiple types of HOI and spans a range of clip durations. All videos have a resolution of at least 720p and include significant motions.</p>
                    </div>
                  </div>
                  <div class="container is-max-desktop">
                    <div class="hero-body">
                        <img src="static/images/fig5.png" alt="Caption words statistics in HOIGen-1M."/>
                    </div>
                    <div class="level-set has-text-justified">
                      <p>The distribution of word numbers shows the captions are high-quality and fine-grained,
                      with an average length of 152 words. The distribution of actions and objects in the captions further demonstrates the diversity of the dataset.
                      There are over 15,000 objects and over 7,000 interaction action types, making it possible to train a T2V model to simulate the real world. For
                      clarity, we have only listed the categories with the highest frequency.</p>
                    </div>
                  </div>
              </div>
          </div>
      </div>
  </div>
  </div>
</section>
<!-- End Dataset -->

<!-- MoME -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <div class="container is-max-desktop"><h2 class="title is-3">Mixture-of-Multimodal-Experts</h2>
        <!-- <div class="columns is-centered has-text-centered"> -->
        <!-- <div class="column is-four-fifths"> -->
          
          <div class="level-set has-text-justified">
            <p>
              To eliminate hallucinations generated by large models in video descriptions, we propose a Mixture-of-Multimodal-Experts Strategy (MoME) to detect hallucinations and then fuse the characteristics of different MLLM to correct them.
            </p>
          <!-- </div> -->

        <!-- </div> -->
        </div>
      
      <div class="container is-max-desktop">
        <div class="hero-body">
            <img src="static/images/fig4.png" alt="An illustration of the Mixture-of-Multimodal-Experts (MoME) strategy-based caption method."/>
        </div>
        <h2 class="level-set has-text-justified">MoME first adopts two captions and one decision expert to detect the hallucination. Then, an additional set of decision experts and caption experts is introduced to eliminate these hallucinations.</h2>
    </div>
    </div>
  </div>
  </div>
</section>
<!-- End MoME -->

<!-- Metrics -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <h2 class="title is-3">New Metrics for HOI</h2>
      <div class="columns is-centered">
          <div class="column is-full">
              <div class="content">
                  <div class="level-set has-text-justified">
                      <p>
                        To accurately align human preference on the HOI video generation, we introduce two new automatic metrics, CoarseHOIScore and FineHOIScore, to assess the visual quality of interaction.
                      <ol>
                          <li><strong>Prompt suite:</strong> we select the object and interaction types from a large-scale HOI detection dataset. Then, we manually filter out rare and unreasonable interactions and get a total of 306 prompts.</li>
                          <li><strong>CoarseHOIScore:</strong> An ideal interaction should at least involve people, objects, and corresponding actions. Therefore, we adopt a HOI detector to predict possible HOI triplets in the generated videos. </li>
                          <li><strong>FineHOIScore:</strong> It evaluates the details of HOIs from a pixel-level perspective, such as the presence or absence of contact between a person and an object and the stability of interactive actions. </li>
                      </ol>
                      </p>
                  </div>
                  <div class="container is-max-desktop">
                    <div class="hero-body">
                        <img src="static/images/table2.png" alt="Evaluation results with proposed HOIScores and VBench in HOI video generation."/>
                    </div>
                    <div class="level-set has-text-justified">
                      <p>Evaluation results with proposed HOIScores and VBench in HOI video generation. Despite the recent significant attention on T2V benchmarks, systematic evaluation of these models on HOI video generation is still lacking. To solve this problem, we evaluate five popular commercial software Kling 1.5, Pika, Hailuo, Dreamina, and Gen-3, as well as five representative open-source methods including OpenSora, OpenSoraPlan, CogVideoX-2B, CogVideoX-5B, and Mochi-10B.</p>
                    </div>
                  </div>
              </div>
          </div>
      </div>
  </div>
  </div>
</section>
<!-- End Metrics -->

<!-- Experiments -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <div class="container is-max-desktop"><h2 class="title is-3">Effectiveness of HOIGen-1M</h2>
      <!-- <div class="columns is-centered has-text-centered"> -->
        <!-- <div class="column is-four-fifths"> -->
          
          <div class="level-set has-text-justified">
            <p>
              We conduct a thorough effectiveness analysis of the proposed captioning method to demonstrate its ability to generate high-quality textual descriptions, which in turn contributes to generating videos that align with HOI.</p>
          </div>

        <!-- </div> -->
      <!-- </div> -->
      
      <div class="container is-max-desktop">
        <div class="hero-body">
            <img src="static/images/table3.png" alt="The effect of the proposed captioning method."/>
        </div>
        <!-- <h2 class="level-set has-text-justified">MoME first adopts two captions and one decision expert to detect the hallucination. Then, an additional set of decision experts and caption experts is introduced to eliminate these hallucinations.</h2> -->
      </div>
      <!-- <div class="columns is-centered has-text-centered"> -->
        <!-- <div class="column is-four-fifths"> -->
          
          <div class="level-set has-text-justified">
            <p>
              We fine-tune three models on the HOIGen-1M using the LoRA mechanism. A significant increase can be observed after fine-tuning in the HOI scores of all three models, which directly demonstrates the effectiveness of HOIGen-1M in generating HOI videos.</p>
          </div>

        <!-- </div> -->
      <!-- </div> -->
      <div class="container is-max-desktop">
        <div class="hero-body">
            <img src="static/images/table4.png" alt="TThe effect of fine-tuning using HOIGen-1M."/>
        </div>
        <h2 class="level-set has-text-justified">The quality of the generated interactive videos significantly improves after fine-tuning the model on HOIGen-1M. Here are the video comparison results:</h2>
      </div>
      <div class="container is-max-desktop">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-video1">
            <video poster="" id="video1" autoplay controls muted loop height="100%">
              <!-- Your video file here -->
              <source src="static/videos/1.mp4"
              type="video/mp4">
            </video>
          </div>
          <div class="item item-video2">
            <video poster="" id="video2" autoplay controls muted loop height="100%">
              <!-- Your video file here -->
              <source src="static/videos/2.mp4"
              type="video/mp4">
            </video>
          </div>
          <div class="item item-video3">
            <video poster="" id="video3" autoplay controls muted loop height="100%">\
              <!-- Your video file here -->
              <source src="static/videos/3.mp4"
              type="video/mp4">
            </video>
          </div>
          <div class="item item-video4">
            <video poster="" id="video4" autoplay controls muted loop height="100%">\
              <!-- Your video file here -->
              <source src="static/videos/4.mp4"
              type="video/mp4">
            </video>
          </div>
          <div class="item item-video5">
            <video poster="" id="video5" autoplay controls muted loop height="100%">\
              <!-- Your video file here -->
              <source src="static/videos/5.mp4"
              type="video/mp4">
            </video>
          </div>
          <div class="item item-video6">
            <video poster="" id="video6" autoplay controls muted loop height="100%">\
              <!-- Your video file here -->
              <source src="static/videos/6.mp4"
              type="video/mp4">
            </video>
          </div>
          <div class="item item-video7">
            <video poster="" id="video7" autoplay controls muted loop height="100%">\
              <!-- Your video file here -->
              <source src="static/videos/7.mp4"
              type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </div>
  </div>
</section>
<!-- End Experiments -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @inproceedings{HOIGen,
          author       = {Kun Liu and
                          Qi Liu and
                          Xinchen Liu and
                          Jie Li and
                          Yongdong Zhang and
                          Jiebo Luo and
                          Xiaodong He and
                          Wu Liu},
          title        = {HOIGen-1M: A Large-scale Dataset for Human-Object Interaction Video Generation},
          booktitle    = {{CVPR}},
          year         = {2025}
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
